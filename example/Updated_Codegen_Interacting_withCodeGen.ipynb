{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updated_Codegen_Interacting_withCodeGen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interacting with [CodeGen](https://github.com/salesforce/CodeGen/)\n",
        "\n",
        "- Originally from afiaka87\n",
        "- Orginized Colab by mega b#6696\n",
        "- Modified by Penguin-jpg"
      ],
      "metadata": {
        "id": "UFoSzgxnvrlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Show gpu information\n",
        "from IPython.display import clear_output\n",
        "from google.colab.output import eval_js\n",
        "eval_js('google.colab.output.setIframeHeight(\"500\")')\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "5VtHpiGpqLWF",
        "outputId": "cb4f8037-046e-4119-8e5b-fb9dacc82cb8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr  4 13:12:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wPwrvvMlkTVA",
        "outputId": "42d9efd6-fee3-4eac-ea45-9c3fcb1775e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CodeGen'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 88 (delta 39), reused 51 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (88/88), done.\n",
            "/content/CodeGen\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-62.0.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 41.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pip-22.0.4 setuptools-62.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2041348096 bytes == 0x55a30d48a000 @  0x7fa4204ef1e7 0x55a30a700407 0x55a30a6ca17c 0x55a30a7aa47a 0x55a30a6ccf9d 0x55a30a7bed4d 0x55a30a740ec8 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a6ce7aa 0x55a30a73cb4f 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73d719 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a73ba2e\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2551685120 bytes == 0x55a386f52000 @  0x7fa4204f0615 0x55a30a6ca17c 0x55a30a7aa47a 0x55a30a6ccf9d 0x55a30a7bed4d 0x55a30a740ec8 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a6ce7aa 0x55a30a73cb4f 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73d719 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a73ba2e 0x55a30a6cef21\n",
            "tcmalloc: large alloc 2041348096 bytes == 0x55a30d48a000 @  0x7fa4204ef1e7 0x55a30a6ff338 0x55a30a6c9ad7 0x55a30a6cbad0 0x55a30a6ccf9d 0x55a30a7bed4d 0x55a30a740ec8 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73cb4f 0x55a30a73ba2e 0x55a30a6cef21 0x55a30a6cf341 0x55a30a83e882 0x55a30a6cd902 0x55a30a741522 0x55a30a6ce7aa 0x55a30a740d30 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a740d30 0x55a30a73ba2e 0x55a30a6ce88a 0x55a30a73d719 0x55a30a7bfb76 0x55a30a73cd95 0x55a30a7bfb76 0x55a30a73cd95 0x55a30a7bfb76 0x55a30a73cd95 0x55a30a6cece9\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m850.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu111->-r requirements.txt (line 2)) (3.10.0.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.2/895.2 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (4.63.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (1.21.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2->-r requirements.txt (line 3)) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2->-r requirements.txt (line 3)) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2->-r requirements.txt (line 3)) (1.15.0)\n",
            "Installing collected packages: tokenizers, torch, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.9.0+cu111 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.0+cu111 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 torch-1.9.0+cu111 transformers-4.16.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#@markdown # Install requirements\n",
        "!git clone https://github.com/salesforce/CodeGen\n",
        "%cd CodeGen\n",
        "!pip install --upgrade pip setuptools\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Choose model checkpoint\n",
        "chosen_model = \"codegen-350M-mono\"#@param [\"codegen-350M-nl\", \"codegen-350M-multi\", \"codegen-350M-mono\", \"codegen-2B-nl\", \"codegen-2B-multi\", \"codegen-2B-mono\", \"codegen-6B-nl\", \"codegen-6B-multi\", \"codegen-6B-mono\", \"codegen-16B-nl\", \"codegen-16B-multi\", \"codegen-16B-mono\"]\n",
        "\n",
        "if chosen_model == \"codegen-350M-nl\":\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-350M-nl.tar.gz && tar -xvf checkpoints/codegen-350M-nl.tar.gz -C checkpoints/\n",
        "elif chosen_model == \"codegen-350M-multi\":\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-350M-nl.tar.gz && tar -xvf checkpoints/codegen-350M-nl.tar.gz -C checkpoints/\n",
        "elif chosen_model == \"codegen-350M-mono\":\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-350M-mono.tar.gz && tar -xvf checkpoints/codegen-350M-mono.tar.gz -C checkpoints/\n",
        "elif chosen_model == \"codegen-2B-nl\":\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-2B-nl.tar.gz && tar -xvf checkpoints/codegen-2B-nl.tar.gz -C checkpoints/\n",
        "elif chosen_model == \"codegen-2B-multi\":\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-2B-multi.tar.gz && tar -xvf checkpoints/codegen-2B-multi.tar.gz -C checkpoints/\n",
        "elif chosen_model == \"codegen-2B-mono\":\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-2B-mono.tar.gz && tar -xvf checkpoints/codegen-2B-mono.tar.gz -C checkpoints/\n",
        "elif chosen_model == \"codegen-6B-nl\":\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-6B-nl.tar.gz && tar -xvf checkpoints/codegen-6B-nl.tar.gz -C checkpoints/\n",
        "elif chosen_model == \"codegen-6B-multi\":\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-6B-multi.tar.gz && tar -xvf checkpoints/codegen-6B-multi.tar.gz -C checkpoints/\n",
        "elif chosen_model == \"codegen-6B-mono\":\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-6B-mono.tar.gz && tar -xvf checkpoints/codegen-6B-mono.tar.gz -C checkpoints/\n",
        "elif chosen_model == \"codegen-16B-nl\":\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-16B-nl.tar.gz && tar -xvf checkpoints/codegen-16B-nl.tar.gz -C checkpoints/\n",
        "elif chosen_model == \"codegen-16B-multi\":\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-16B-multi.tar.gz && tar -xvf checkpoints/codegen-16B-multi.tar.gz -C checkpoints/\n",
        "else:\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-16B-mono.tar.gz && tar -xvf checkpoints/codegen-16B-mono.tar.gz -C checkpoints/"
      ],
      "metadata": {
        "id": "xZbWFH1hkf9i",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b8d9d7-5fa7-40b9-8cfa-3969725f0b35"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-04 13:24:18--  https://storage.googleapis.com/sfr-codegen-research/checkpoints/codegen-350M-mono.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.133.128, 74.125.140.128, 108.177.15.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.133.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 656968931 (627M) [application/x-tar]\n",
            "Saving to: ‘checkpoints/codegen-350M-mono.tar.gz’\n",
            "\n",
            "codegen-350M-mono.t 100%[===================>] 626.53M  15.6MB/s    in 40s     \n",
            "\n",
            "2022-04-04 13:24:59 (15.6 MB/s) - ‘checkpoints/codegen-350M-mono.tar.gz’ saved [656968931/656968931]\n",
            "\n",
            "codegen-350M-mono/\n",
            "codegen-350M-mono/config.json\n",
            "codegen-350M-mono/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before executing this code please update the `sample.py` file\n",
        "\n",
        "# => please add this path in sample.py file\n",
        "```python\n",
        "import sys\n",
        "# insert at 1, 0 is the script path (or '' in REPL)\n",
        "sys.path.insert(1, '/content/CodeGen/jaxformer/hf/codegen/')\n",
        "\n",
        "# Comment this line\n",
        "# from ..jaxformer.hf.codegen.modeling_codegen import CodeGenForCausalLM  \n",
        "import modeling_codegen\n",
        "```\n",
        "and replace `CodeGenForCausalLM` with `modeling_codegen.CodeGenForCausalLM`\n",
        "Same update you have to do in modeling_codegen.py in entire code.\n",
        "\n",
        "```python\n",
        "import sys\n",
        "# insert at 1, 0 is the script path (or '' in REPL)\n",
        "sys.path.insert(1, '/content/CodeGen/jaxformer/hf/codegen/')\n",
        "import configuration_codegen\n",
        "```\n",
        "\n",
        "Replace `CodeGenConfig` with `configuration_codegen.CodeGenConfig` in entire code."
      ],
      "metadata": {
        "id": "vwpW1UeYqKis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/CodeGen/\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBFailRQspLo",
        "outputId": "dafa316a-a14c-42f0-d72d-bd64f2eb8c08"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\t     CODE_OF_CONDUCT.md  LICENSE.txt  requirements.txt\n",
            "checkpoints  jaxformer\t\t README.md    SECURITY.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Try out the model.\n",
        "rng_seed = 42 #@param {type:\"integer\"}\n",
        "p = 0.95 #@param {type:\"number\"}\n",
        "t = 0.2 #@param {type:\"number\"}\n",
        "max_length =  200#@param {type:\"integer\"}\n",
        "batch_size = 1 #@param {type:\"integer\"}\n",
        "context = \"class palindrom():\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "!python3 -m jaxformer.hf.sample --model $chosen_model \\\n",
        "                 --rng-seed $rng_seed \\\n",
        "                 --p $p \\\n",
        "                 --t $t \\\n",
        "                 --max-length $max_length \\\n",
        "                 --batch-size $batch_size \\\n",
        "                 --context '$context'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN2xY4xmkss0",
        "outputId": "a0a1c2b1-6508-46de-cd53-7acea1e201a2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading parameters\n",
            "loading parameters took 16.11s\n",
            "loading tokenizer\n",
            "loading tokenizer took 2.87s\n",
            "sampling\n",
            "====================================================================================================\n",
            "\n",
            "    def __init__(self, a, b):\n",
            "        self.a = a\n",
            "        self.b = b\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        return self.a == other.a and self.b == other.b\n",
            "\n",
            "    def __ne__(self, other):\n",
            "        return self.a!= other.a or self.b!= other.b\n",
            "\n",
            "    def __repr__(self):\n",
            "        return '%s(%r, %r)' % (self.__class__.__name__, self.a, self.b)\n",
            "\n",
            "    def __str__(self):\n",
            "        return '%s(%r, %r)' % (self.__class__.__name__, self.a, self.b)\n",
            "\n",
            "    def __hash__(self):\n",
            "        return hash((self.a, self.b))\n",
            "\n",
            "    def __lt__(self\n",
            "====================================================================================================\n",
            "class palindrom():\n",
            "    def __init__(self, a, b):\n",
            "        self.a = a\n",
            "        self.b = b\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        return self.a == other.a and self.b == other.b\n",
            "\n",
            "    def __ne__(self, other):\n",
            "        return self.a!= other.a or self.b!= other.b\n",
            "\n",
            "    def __repr__(self):\n",
            "        return '%s(%r, %r)' % (self.__class__.__name__, self.a, self.b)\n",
            "\n",
            "    def __str__(self):\n",
            "        return '%s(%r, %r)' % (self.__class__.__name__, self.a, self.b)\n",
            "\n",
            "    def __hash__(self):\n",
            "        return hash((self.a, self.b))\n",
            "\n",
            "    def __lt__(self\n",
            "====================================================================================================\n",
            "sampling took 16.44s\n",
            "done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Fibonacci() :\n",
        "    a, b = 0, 1\n",
        "    while True :\n",
        "        yield a\n",
        "        a, b = b, a+b\n",
        "\n",
        "f = Fibonacci()\n",
        "print(\"Fibobacci Series:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(next(f), end=\",\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT9M_d1Fn4Bl",
        "outputId": "d173acf3-ee83-4a2b-cb72-9dc5a4a07aee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fibobacci Series: 0,1,1,2,3,5,8,13,21,34,"
          ]
        }
      ]
    }
  ]
}